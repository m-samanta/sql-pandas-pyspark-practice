{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem:**  \n",
    "You have a DataFrame `df` with the following data:\n",
    "\n",
    "| transaction_id | product_id | quantity | price |\n",
    "|----------------|------------|----------|-------|\n",
    "| 1              | 1          | 2        | 1000  |\n",
    "| 2              | 2          | 3        | 500   |\n",
    "| 3              | 1          | 1        | 1000  |\n",
    "| 4              | 3          | 4        | 300   |\n",
    "\n",
    "**Task:** Calculate the total sales amount for each product. Ensure that each product appears only once in the result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    'transaction_id': [1, 2, 3, 4],\n",
    "    'product_id': [1, 2, 1, 3],\n",
    "    'quantity': [2, 3, 1, 4],\n",
    "    'price': [1000, 500, 1000, 300]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df['total_sales'] = df['quantity'] * df['price']\n",
    "result = df.groupby('product_id')['total_sales'].sum().reset_index()\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem:**  \n",
    "You have a PySpark DataFrame `df` with the following columns:\n",
    "\n",
    "| order_id | customer_id | order_date        | amount |\n",
    "|----------|-------------|-------------------|--------|\n",
    "| 1        | 101         | 2024-08-01 12:00  | 200    |\n",
    "| 2        | 102         | 2024-08-02 14:00  | NULL   |\n",
    "| 3        | 101         | 2024-08-03 09:00  | 300    |\n",
    "| 4        | 103         | 2024-08-04 10:00  | NULL   |\n",
    "\n",
    "**Task:** Fill the missing values in the `amount` column with the average amount for each customer. Then, calculate the running total of the amount for each customer, ordered by `order_date`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "data = [\n",
    "    (1, 101, '2024-08-01 12:00', 200),\n",
    "    (2, 102, '2024-08-02 14:00', None),\n",
    "    (3, 101, '2024-08-03 09:00', 300),\n",
    "    (4, 103, '2024-08-04 10:00', None)\n",
    "]\n",
    "\n",
    "columns = ['order_id', 'customer_id', 'order_date', 'amount']\n",
    "df = spark.createDataFrame(data, schema=columns)\n",
    "\n",
    "avg_amount_df = df.groupBy('customer_id').agg(avg('amount').alias('avg_amount'))\n",
    "df_filled = df.join(avg_amount_df, on='customer_id', how='left')\n",
    "df_filled = df_filled.withColumn('amount', when(col('amount').isNull(), col('avg_amount')).otherwise(col('amount')))\n",
    "\n",
    "window_spec = Window.partitionBy('customer_id').orderBy('order_date').rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "df_result = df_filled.withColumn('running_total', sum(col('amount')).over(window_spec))\n",
    "\n",
    "df_result.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
