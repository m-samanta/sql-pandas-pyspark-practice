{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pandas Problem:**\n",
    "\n",
    "**Problem:**  \n",
    "You have a DataFrame `df` with the following data:\n",
    "\n",
    "| employee | department | salary |\n",
    "|----------|------------|--------|\n",
    "| Alice    | HR         | 50000  |\n",
    "| Bob      | Engineering| 60000  |\n",
    "| Charlie  | HR         | 52000  |\n",
    "| David    | Marketing  | 55000  |\n",
    "\n",
    "**Task:** Calculate the average salary for each department."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "employees = {\n",
    "    'employee': ['Alice', 'Bob', 'Charlie', 'David'],\n",
    "    'department': ['HR', 'Engineering', 'HR', 'Marketing'],\n",
    "    'salary': [50000, 60000, 52000, 55000]\n",
    "}\n",
    "df = pd.DataFrame(employees)\n",
    "\n",
    "average_salary_per_department = df.groupby('department')['salary'].mean()\n",
    "\n",
    "print(average_salary_per_department)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PySpark Problem:**\n",
    "\n",
    "**Problem:**  \n",
    "You have a PySpark DataFrame `df` with the following columns:\n",
    "\n",
    "| order_id | order_date        | amount |\n",
    "|----------|-------------------|--------|\n",
    "| 1        | 2024-08-01 12:00  | 200    |\n",
    "| 2        | 2024-08-02 14:00  | 150    |\n",
    "| 3        | 2024-08-03 09:00  | 300    |\n",
    "\n",
    "**Task:** Convert the `order_date` column to a date type and extract the month from it. Then, count the number of orders per month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import to_date, month, count\n",
    "\n",
    "spark = SparkSession.builder.appName(\"OrderAnalysis\").getOrCreate()\n",
    "\n",
    "data = [\n",
    "    (1, \"2024-08-01 12:00\", 200),\n",
    "    (2, \"2024-08-02 14:00\", 150),\n",
    "    (3, \"2024-08-03 09:00\", 300)\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"order_id\", \"order_date\", \"amount\"])\n",
    "\n",
    "df = df.withColumn(\"order_date\", to_date(df[\"order_date\"], \"yyyy-MM-dd HH:mm\"))\n",
    "\n",
    "df = df.withColumn(\"month\", month(df[\"order_date\"]))\n",
    "\n",
    "monthly_counts = df.groupBy(\"month\").agg(count(\"order_id\").alias(\"order_count\"))\n",
    "\n",
    "monthly_counts.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
