{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem:**  \n",
    "You have a DataFrame `df` with the following data:\n",
    "\n",
    "| user_id | email              |\n",
    "|---------|---------------------|\n",
    "| 1       | john.doe@example.com|\n",
    "| 2       | alice.smith@sample.org |\n",
    "| 3       | john.doe@example.com|\n",
    "| 4       | bob_jones@mail.net  |\n",
    "\n",
    "**Task:** Use a regular expression to extract the domain names from the email addresses, then find the distinct domain names in the DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'user_id': [1, 2, 3, 4],\n",
    "    'email': ['john.doe@example.com', 'alice.smith@sample.org', 'john.doe@example.com', 'bob_jones@mail.net']\n",
    "})\n",
    "\n",
    "df['domain'] = df['email'].str.extract(r'@([^.]+)\\.')\n",
    "distinct_domains = df['domain'].unique()\n",
    "\n",
    "print(distinct_domains)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem:**  \n",
    "You have a PySpark DataFrame `df` with the following columns:\n",
    "\n",
    "| transaction_id | customer_id | amount |\n",
    "|----------------|-------------|--------|\n",
    "| 1              | 101         | 500    |\n",
    "| 2              | 102         | 300    |\n",
    "| 3              | 101         | 700    |\n",
    "| 4              | 103         | 200    |\n",
    "\n",
    "**Task:** \n",
    "1. Add a new transaction for `customer_id` 104 with an amount of 400.\n",
    "2. Update the amount for `customer_id` 102 to 350.\n",
    "3. Calculate the total amount spent by each customer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import when, sum as spark_sum\n",
    "\n",
    "spark = SparkSession.builder.appName(\"example\").getOrCreate()\n",
    "\n",
    "data = [\n",
    "    (1, 101, 500),\n",
    "    (2, 102, 300),\n",
    "    (3, 101, 700),\n",
    "    (4, 103, 200)\n",
    "]\n",
    "columns = ['transaction_id', 'customer_id', 'amount']\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "new_data = [(5, 104, 400)]\n",
    "new_df = spark.createDataFrame(new_data, columns)\n",
    "df = df.union(new_df)\n",
    "\n",
    "df = df.withColumn(\n",
    "    'amount',\n",
    "    when(df.customer_id == 102, 350).otherwise(df.amount)\n",
    ")\n",
    "\n",
    "result = df.groupBy('customer_id').agg(spark_sum('amount').alias('total_amount'))\n",
    "result.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "daproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
